---
title: "Putting a price tag on football players"
author: "Bjørn Skeel-Gjørling, Chritian Lund Sørensen, Guillaume Slizewicz & Amer Skaljic"
date: "26 August 2016"
output: 
  pdf_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    number_sections: yes
fontsize: 12pt
linestretch: 1.5

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo = FALSE)

##LIBRARIES
library(plotly)
library("ggmap")# getting maps and coordinates from google
library(maptools)# getting maps and coordinates from google
library(maps)# getting maps and coordinates from google
library("ggplot2")# plotting the data
library(dplyr)#tidying dataset
library(rworldmap)
library(stringr)#dealing with strings and replacing strings in observations
library(RCurl)
library(extrafont)
library(randomForest)
library(tree)
loadfonts()

#creating dataset
df.viz<- read.csv("https://raw.githubusercontent.com/basgpol/SDS-group12/master/Exam_project/transferdata.final.csv", encoding = "UTF8", header = TRUE)

####remove player with transfer fee= 0 et contract time left=0
df.viz<- filter(df.viz,transfer.fee>0)

##GETTING STARTING MAP
myLocation <- "Zurich"
myMap <- get_map(location= myLocation,
                 source="stamen",maptype="toner", crop=FALSE,zoom=4)
##########
##########  MAP CLUB 
##########

#GETTING DATA
df.spending.club<-read.csv("https://raw.githubusercontent.com/basgpol/SDS-group12/master/Exam_project/df_spending_club_with_geo.csv", encoding = "UTF8", header = TRUE)

#WITH GGPLOT
map.clubs <- ggmap(myMap) +
  geom_point(aes(x = lon, y = lat, size=transfer.fee.total), data =df.spending.club,col="red", alpha=0.4)+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks= element_line(color=NA),
        axis.line = element_line(color = NA),
        text=element_text(family="Goudy Old Style"))+
  ggtitle("Total transfer spending for clubs in Europe")+
  labs(size="")+
  scale_size(breaks = c(5.0e+7,1.0e+8,1.5e+8),labels = c("50M£","100M£","150M£"))

##########
##########  MAP TRANSFER 
##########

transfer.path.full = read.csv("https://raw.githubusercontent.com/basgpol/SDS-group12/master/Exam_project/transfer_path_full.csv", header=TRUE, stringsAsFactors=TRUE, fileEncoding="UTF8") # loading saved version of uncleaned player data

##MAAPING 

path.map<-ggmap(myMap)+#calling map
  geom_path(aes(x = lon, y = lat, group = factor(name)), #putting paths on the map
            colour="red", data = transfer.path.full, alpha=0.4)+
  theme(axis.title.x=element_blank(),
        axis.ticks= element_line(color=NA),
        axis.text= element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        panel.grid.major.y = element_line(colour="#CACACA", size=0.2), #add grid
        axis.title.y=element_blank(),
        text=element_text(family="Goudy Old Style"))+
  ggtitle("Transfers for season 2014/2015\n")

##########
##########  MAP TRANSFER 
##########

transfer.path.full = read.csv("https://raw.githubusercontent.com/basgpol/SDS-group12/master/Exam_project/transfer_path_full.csv", header=TRUE, stringsAsFactors=TRUE, fileEncoding="UTF8") # loading saved version of uncleaned player data


##MAAPING 

path.map<-ggmap(myMap)+#calling map
  geom_path(aes(x = lon, y = lat, group = factor(name)), #putting paths on the map
            colour="red", data = transfer.path.full, alpha=0.4)+
  theme(axis.title.x=element_blank(),
        axis.ticks= element_line(color=NA),
        axis.text= element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        panel.grid.major.y = element_line(colour="#CACACA", size=0.2), #add grid
        axis.title.y=element_blank(),
        text=element_text(family="LM Roman 10"))+
  ggtitle("Transfer for season 2014/2015")

##########
########## MAP TRANSFER+CLUB 
##########

full.map<-ggmap(myMap)+#calling map
  geom_path(aes(x = lon, y = lat, group = factor(name)), #putting paths on the map
            colour="orange", data = transfer.path.full, alpha=0.4)+
  geom_point(aes(x = lon, y = lat, size=transfer.fee.total), data =df.spending.club,col="red", alpha=0.4)+
  theme(axis.title.x=element_blank(),
        axis.ticks= element_line(color=NA),
        axis.text= element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        panel.grid.major.y = element_line(colour="#CACACA", size=0.2), #add grid
        axis.title.y=element_blank(),
        text=element_text(family="Goudy Old Style"))+
  ggtitle("Transfer for season 2014/2015\n")+
  labs(size="Transfers spending\n per club")+
  scale_size(breaks = c(5.0e+7,1.0e+8,1.5e+8),labels = c("50M£","100M£","150M£"))

##########
##########  SCATTER PLOT+AGE
##########

p.age = ggplot(df.viz, aes(x = transferage , y = transfer.fee))
p.age<-p.age + geom_point(stat = "identity",col="red",alpha=0.4,aes(text = paste("Name:",name)))+ #to use for ggplot
          geom_smooth(aes(colour = transferage, fill = transferage))+
          ggtitle("Age repartition of transfers in European leagues\n")+
  labs(y="Transfer price\nin M£",x="Age") +
  theme(axis.ticks.y= element_line(color=NA),
        axis.ticks.x=element_line(colour="#CACACA", size=0.2),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.title.y =element_text(angle = 0,
                                   colour="#525252",
                                   vjust = 1,
                                   hjust = 0 ),
        axis.title.x =element_text(angle = 0,
                                   colour="#525252",
                                   vjust = -1,
                                   hjust = 1 ),
        panel.grid.major.y = element_line(colour="#CACACA", size=0.2), #add grid
        axis.title.y=element_blank(),
        text=element_text(family="Goudy Old Style"))

##########
##########  SCATTER PLOT+TIME LEFT
##########

p.time <- ggplot(data=df.viz, aes(x = contract.left.month , y = transfer.fee)) +
  geom_point(stat = "identity",col="red",alpha=0.4,aes(text = paste("Name:",name)))+
  #geom_point(aes(text = paste(name, " to ", club.to)), size = 4) +
  geom_smooth(aes(colour = contract.left.month, fill = contract.left.month))+
  ggtitle("Time left on contract seems to be positively correlated with transfer fees\n")+
  labs(y="Transfer price\nin M£",x="Time left on contract\nin months")+
  scale_x_continuous(breaks=seq(0,58,12))+
  theme(axis.ticks.y= element_line(color=NA),
        axis.ticks.x=element_line(colour="#CACACA", size=0.2),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.title.y =element_text(angle = 0,
                                   colour="#525252",
                                   vjust = 1,
                                   hjust = 0 ),
        axis.title.x =element_text(angle = 0,
                                   colour="#525252",
                                   vjust = -1,
                                   hjust = 1 ),
        panel.grid.major.y = element_line(colour="#CACACA", size=0.2),
        panel.grid.major.x =element_blank(), #add grid
        axis.title.y=element_blank(),
        text=element_text(family="Goudy Old Style"))

##########
##########  BAR CHART+AVERAGE CLUB SPENDING
##########


#finding mean for every league
df.viz.ave<-df.viz %>%
  group_by(league) %>%
  dplyr::summarise(
    sum.transfer=sum(transfer.fee))

df.viz.ave<-df.viz.ave %>%
  mutate(number.of.clubs = ifelse(league=="Bundesliga",18,20)) %>%
  mutate(average.spending=sum.transfer/number.of.clubs)
#ordering
df.viz.ave <- transform(df.viz.ave,
                        league = reorder(league, average.spending))
#plotting it
total.league = ggplot(df.viz.ave, aes( x =league, y=average.spending, fill=league))
total.league<-total.league + geom_bar(stat="identity",alpha=1)+
  theme(axis.title.x=element_blank(),
        axis.title.y =element_text(angle = 0,
                                   colour="#525252",
                                   vjust = 1,
                                   hjust = 0 ),
        panel.grid.major.y = element_line(colour="#CACACA", size=0.2), #add grid
        axis.ticks= element_line(color=NA),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position="none",
        panel.background = element_blank(),
        text=element_text(family="LM Roman 10"))+
  ggtitle("Premier League Clubs spend far more on average\n than any other leagues' clubs\n")+
  scale_fill_manual("National leagues",
                    values=c( "#F2E7DA", "#E89090","#92A0B0", "#C0CFAE", "#525252", "#DEE3DC"))+
  labs(y="Average transfer spending\nper league in M£")

##########
##########  BAR CHART+AVERAGE PRICE PER LEAGUE
##########

#finding mean spending per player for different leagues
df.viz.player.league<-df.viz %>%
  group_by(league) %>%
  dplyr::summarise(average.spending.per.player=mean(transfer.fee))

#ordering
df.viz.player.league <- transform(df.viz.player.league,
                        league = reorder(league, average.spending.per.player))
#plotting it
ave.player = ggplot(df.viz.player.league, aes( x =league, y=average.spending.per.player, fill=league))
ave.player<-ave.player + geom_bar(stat="identity",alpha=1)+
  theme(axis.title.x=element_blank(),
        panel.grid.major.y = element_line(colour="#CACACA", size=0.2), #add grid
        axis.ticks= element_line(color=NA),
        panel.grid.major.x = element_blank(),
        axis.title.y =element_text(angle = 0,
                                   colour="#525252",
                                   vjust = 1),
        panel.grid.minor = element_blank(),
        legend.position="none",
        panel.background = element_blank(),
        legend.position="none",
        text=element_text(family="Goudy Old Style"))+
  ggtitle("Premier League Clubs spend far more on average\n than any other leagues' clubs\n")+
  scale_fill_manual(
                    values=c( "#E89090","#F2E7DA","#C0CFAE","#92A0B0",  "#525252", "#DEE3DC"))+
  labs(y="Average transfer price\nper player in M£")


##########
##########  BAR CHART+AVERAGE PRICE PER CLUB STATUS
##########

df.viz.status<-df.viz %>% 
  group_by(Status) %>% 
  dplyr::summarise(mean.transfer=mean(transfer.fee))

#Ordering
df.viz.status <- transform(df.viz.status, 
                                  Status = reorder(Status, mean.transfer))

#Plotting

p.club = ggplot(df.viz.status, aes(y= mean.transfer, x = Status, fill=Status))
p.club<- p.club + geom_bar(stat = "identity")+
  theme(axis.title.x=element_blank(),
        axis.text.x =element_text(size  = 9),
        axis.title.y =element_text(angle = 0,
                                   colour="#525252",
                                   vjust = 1),
        axis.ticks= element_line(color=NA),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(colour="#CACACA", size=0.2), #add grid
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        legend.text=element_blank(),
        legend.title=element_blank(),
        legend.position="none",
        text=element_text(family="Goudy Old Style"))+
  ggtitle("Top Club spend far more on average than other leagues' clubs\n")+
  scale_fill_manual(values=c( "#CFF09E", "#A8DBA8", "#79BD9A", "#3B8686"))+
  labs(y="Average transfer price\nper player in M£") 


```

# Introduction

The purpose of this papers is to predict the transfervalue of football players in the five major european leagues. We use a variety of methods from the field of data science to gahter, descipe and lastly predict footballers transfer value. The paper is structured into three sections: Data gathering, data description using different visualizations and prediction modelling.[^1] In the first section we briefly describe how we scraped the data from diferent webpages, what type of data emerged and what ethical challenges we should consider. In the second section the dataset is described using vizualization in order to get an overview of the data and the variables which seems to affect the transfer values. In the final setion, we use statistical learning models to predict the transfer values of footballers. The emphasis will be on testing different models and comparing their predictive power on a test set.

[^1]: All calculations, graphics and writing was conducted using the programming language [R](https://www.r-project.org/) and the IDE [RStudio](https://www.rstudio.com/). 

# Data Gathering

In order to do the analysis, we have to gather data. We gathered the relevant data from the website Transfermarkt.co.uk and Wikipedia.org. Transfermarkt.co.uk contains information on football transfers, player's information and statistics. We find all this kind of information highly relevant in order to predict the transfer value of a player.
We used Wikipedia.org in order to find data about the table rangings in the five leagues, which also was available on Transfermarkt.co.uk but we could not succeed in scraping it from the webpage.
The data was extracted by two web scrapers, one for Transfermarkt.co.uk and the other for Wikipedia.org. The two scrapers was biuld with the purpose of scraping the relevant data on both webpages. The scraper for Transfermarkt.co.uk ran through all transfers in the transfer windows "Summer 15" and "Winter 16" in the five biggest European leagues: Premier League, Bundesliga, La Liga, Serie A and Ligue 1. It also ran through the transfered players' data. This means that we had two dataframes from Transfermarkt.co.uk at first, which then was merged into one dataframe consisting of players' data.
The scraper for Wikipedia.org ran through the different webpages with the ranging tables of the five leagues. At first we had 5 different dataframes, which we merged into one dataframe.
The two different datasets were at first cleaned and then merged into one big dataset containing only the variables we find relevant in predicting the transfer values of the players.
We do not consider any ethical issues scrapping this data, because all the information is publicly available and not private in any way.

## Data Cleaning

### Cleaning The Player Dataset
As written earlier before merging the different dataframes into one final, we had to clean both the dataframe with the player data and the dataframe with the club data. Cleaning the player data, we first had to make all unavailable data into NA in stead of different signs. We also had to clean the transfer date and put it into the right format, which we also did with the variable containing remaining time under contract. Some of the data scraped from Transfermarkt.co.uk had some mistakes, which we had to fix by putting them to be equal to NA.
We calculated the age of each player at the transferdate by first cleaning the birth date of the players and afterwards calculating the transferage in R by our selves.
At last we divided the players into three different categories: Defender, Midfield and Attacker because we found it more appropriate in order to do our analysis. If we did not divide the players into these categories, we would have a dataset with a lot of different type of players, which could be categorized as we did. This categorization means that it is possible to do a reasonable vizualizaton considering that we "only" have 12 pages to write our project.
We removed all duplicated observations, observations where transfer.fee was NA and the keepers. The reason for removing the keepers was because we scraped data on mostly offensive statistics, which obviously does not have an affect on the keepers' transfer values. Also the number of variables concerning the keepers' was different compared to the other three types of players, which meant that the observations did not fit into the right column for the keepers.

### Cleaning The Club Dataset

Cleaning the club dataset we started with grouping the different clubs in 3 categories: Top Club, Middle Club and Bottom Club. Afterwards we renamed the clubs, so they matched the player datasetm which meant that we could merge the player dataset and the club dataset by using leftjoin.
At last, before merging, we selected the variables that we found interesting and removed all other variables in our final cleaned club dataset.

## The Final Dataset

The clean and merged data set contains the following descibtive variables of the transfered players:

Variable Name | Describtion
------------- | -------------
name          | Name of player
nationality   | Nationality of player
birth_place   | Birth place of player
birth.date    | The date at which the player was born
transferage   | Age of player when transfer occured

The clean and merged data set contains the following statistial variables of players:

Variable Name         | Describtion
--------------------- | -------------
positions             | Position on field of player
total.goals           | Total amount of goals scored by player
penaltygoals          | Total amount of penalty goals scored by player
total.assists         | Total amount of assists made by player
substitutions_in      | Total amount of matches where player gets substituted into the match
substitutions_out     | Total amount of matches where player gets substituted out of the match
total.minutes.played  | Total amount of minutes played by player
minutes.pr.goal       | Amount of minutes played per goal scored by player
yellowcards           | Total amount of yellow cards the player got
secondyellow          | Total amount of second yellow cards the player got
redcards              | Total amount of red cards the player got
contract.left.month   | How many month left of the transfered players' contract when transfer occurs

All the statistics above are from the season before the transfer happend, which means the season 2014/2015.

The clean and merged data set contains the following descibtive variables of clubs and transfers:

Variable name | Describtion
------------- | -------------
club.to       | Which club player is transfered to
club.from     | Which club player is transfered from
league        | The league at which the buying club is playing
status        | Status of the buying club, which is divided in "Top Club", "Middle Club" and "Bottom Club"
transfer.date | Which date the transfer happened
transfer.fee  | The transfer fee in million of pounds


All the variables above, we think, are relevant in order to make the best prediction possible of a players transfer value.   

# Data visualisation

The objective of this section is to highlights possible correlation, clear outliers, important variable via the use of data visualisation processes. The creation of maps, scatter plots and bar charts enabled us to have a better understanding of our dataset and convey its most important characteristic in an engaging and simple way. Besides, after experimenting different scatterplots, we were able to have a grasp of the relationship between transfer fees and other variables such as age, position, appearance, total goals and time left on contract.  
In order to transform our data set into graphs and maps we used different digital tools: GGplot for scatterplots and bar chart, Google maps API for maps, and plotly for interactive visualisation. The dataset used in this visualisations was cleaned by filtering all observations with a transfer fee equal to zero, this resulted in clearer representations.

## A European market dominated by the UK.

&nbsp;

```{r, echo=FALSE, message = FALSE}


map.clubs

```

&nbsp;

Our first representation was a mapping of all the clubs buying players with the size of the plots corresponding with their transfer spendings. This allowed us to check the validity of our sample, by comparing it to the most well known clubs and gave us a better overview of the big player in the market. Unsurprisingly, the premier league appeared to have on average more clubs with a higher spending, and the most reknown clubs such as PSG, Real Madrid or Barcelona clearly appeared on the map.

&nbsp;

```{r, echo=FALSE, message = FALSE}


path.map
```

&nbsp;

Our second visualisation focused on transfer path of each player. This highlighted the fact that the market for football players is a European market, or even a world one and that barriers between countries don't seem to play a big role in buying players. As you can see on the map below, most of the players are traded between countries and not within them, confirming the idea that these goods are traded between mostly countries and not within them.


##A foreseeable repartition of player ages around 25

The scatterplot graph of the relationship between Age and transfer fee presents a convex average curve with a flat top around 25. Perhaps forseeably, most players are traded while in their prime, between 20 and 27 years old. It could be interpreted that the the promises of a rising stars are more sought after than the celebrity of an older player. On top of that, this graph shows that transfer above 40 millions are the exception.

&nbsp;

```{r, echo=FALSE, message = FALSE}


p.age

```
  
## Time left on contract/Transfer fees

This graph seems to highlight a correlation between the time left on a player's contract and his transfer fee. The longer time there is left of the contract at the time of the transfer, the more expensive the player will be. When the period is less than 20 months, the transfer fees does not exceed 30 million pounds. When above 20 months, the transfer fees can go up to 30 million pounds - some transfer fees even reach 40 million pounds, when the time left on the contract exceeds 40 months. 

&nbsp;

```{r , echo=FALSE, message = FALSE}


p.time

```
  
## The Premier League spends almost three time as much as the Serie A on new players

&nbsp;

```{r , echo=FALSE, message = FALSE}


total.league

```
  
&nbsp;

This chart shows the spending on transfers in the 5 major leagues in Europe. Average spending are the lowest in the French Ligue 1.The German Bundesliga, the Spanish La Liga and the Italian Serie A all have bigger spendings on average than Ligue 1. We also see that the English Premier League dominates the chart with the highest amount of money spent on average. It amounts up to almost free time the average club budget for the second league in terms of spending, the Serie A.

One of the reason of the Premier League hegemony might be its popularity all over the world, resulting in higher TV licensing rights, bigger sales of fan merchandise and a higher attendance during british match. Both being able to buy expensive players and the ambition to remain at the center of international football push british clubs into buying more, and at a higher price, as shown below. It is difficult to say however if this loops is not feeding itself, and if clubs and agents ask for more money from an english club for the same player because they know they can afford it.(!It would be nice to include club budgets as a variable to know more on this phenomenon)  

## British clubs spend more money per players

&nbsp;

```{r, echo=FALSE, message = FALSE}

ave.player

```

&nbsp;
    
This graph is not as clear cut as the last one. Premier League is still far ahead other clubs, but Spanish teams actually are closer to spending as much as the Premier League for players. This can be explained by the top teams in Spain: Real Madrid, FC Barcelona and Atlético Madrid. Those three clubs spend a lot of money on transfers and try to secure record deals to boost their audience and gain more media coverage. However, this behavior is not followed by other clubs in the league, and there are less player tranfers in total as compared to the premier league. 

## Top Clubs drive the market
  
This graph simply shows that the more successful the club is in football, the more money it can afford to spend on transfers. One interesting feature of this graph however is that the promoted clubs are spending a bit more money on transfer than the bottom clubs. This can be explained by their intention to improve their roster in order to be able to compete in the major league. On the other hand, some of the bottom clubs are relegated, which means that they loose money and therefore cannot afford to spend much. On top of that they will compete in the second best league and therefore have a smaller need for transfers.

&nbsp;

```{r, echo=FALSE, message = FALSE}


p.club
```
 
&nbsp;
 
# Prediction models for transfervalue

## Prediction versus causality
The main purpose of this rapport is to find the best way of predicting the transfervalue of footballers. We therefore focus on finding the best way to estimate our dependent variable (Y) when we know the value of several different indendent variables (X's) - hereafter called predictors. The issue of prediction stand in contrast to the regular goal in social sciences where we want to estimate the causal effect of one particular independent variable on the dependent variable. The key difference is that we don't care that much about the individual effect of a predictor (effects of causes) but instead focus the interplay between several predictors and how this interplay can help us predict the value of Y most efficient. Due to this scientific purpose we use the predictors as signals of a specific transfervalue and don't care about whether they come before Y in time or whether there is omitted variable bias (?). A good example of the above mentioned is our use of google searches as a predictor for the players transfervalue. The amount of searches are recorded in real time and therefore after the transfer was made. The number of google searches can not be the reason why the club bought the particular player for a given price but  a high number of searches can signal that the players has had a succesful career or was part of a news generating transfer.        

## Evaluating prediction models using RMSE
To find the best way of predicting a football players transfervalue we will in the following make a comparative analysis of five different models where we compare the prediction accuracy of each model. The models we will use to predict the transfervalues are 1) simple average, 2) ordinary least square (linear model), 3) lasso model, 4) decision tree and 5) random forest.  

We measure the models' prediction accuracy by finding the *out of sample error* (the prediction error when applying the models on a dataset on which the model was not trained). The first step of our comparative analysis is therefore to randomly split our data into a *training sample* and a *test sample*. We choose to put 70 percent of the observations in the training sample because we want a sufficiently large amount of observations to train our models.  
We evaluate and compare the models by using the root mean squared error (RMSE) which is stated in the equation below. The RMSE gives us a measure for how well the model predict the observations in the test sample. We created a function in R to make the RMSE calculation for us. (We use the RMSE instead of the more basic mean square error (MSE) because RMSE reduces the influnce of outliers.) 

$$ RMSE = \sqrt {{\frac{{\sum\limits_{{i = 1}}^n {{{\left( {{y_i} - {{\hat{y}}_i}} \right)}^2}} }}{{n}}}}$$

A basic trade-off to take into account when dealing with prediction is the *bias-variance trade-off*. The reason is that the prediction errors is the sum of errors due to bias and errors due to variance between the training and test sample. When we build our prediction models we do it on the availiable data in the training sample which also include standard noise. If we on the one hand build a model with only a few variables the model will not be able to comprehent the complexity of the real world. Due to this *underfitting* we therefore end up with a biased model whos estimates are quite far from the real world values. If we on the other hand increase the number of variables and thereby the models complexity alot then the models will adjust to much to the training sample. The result of this *overfitting* will be prediction errors due to the variance between the training sample and the real world. The take-away point is therefore that the most accurate models have found a balance between over- and underfitting so that the increase in bias is equivalent to the decrease in variance. The trade-off is illustrated below:

![Image is from Roe 2016)](Exam_project/biasvariance.png)

Find graph here: http://scott.fortmann-roe.com/docs/BiasVariance.html 

## Simple Average
As the first prediction model we use the simple transfer fee average. This is a good baseline model because it is the most simple way of estimating a players transfer value without using any predictors. When we use the mean transfer fee in the training sample on the test sample we get a RMSE at 9.06.

Because the simple average is not using any predictor for the estimation we can not expect that it will do a very good job at predicting the player's individual transfer value. The high amount of prediction errors is illustrated below:

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(plotly)
library(ggplot2)
library(glmnet)
library(dplyr)
library(plotly)
library(randomForest)

## Loading the final data set
transfer.data = read.csv("https://raw.githubusercontent.com/basgpol/SDS-group12/master/Exam_project/transferdata.final.csv", encoding = "UTF8", header = TRUE)
## Creating a new variable which is age squared
transfer.data$transferage_sq = transfer.data$transferage^2

## creating a vector with selected predictors for transferfee ()
predicting.var = c("transfer.fee", "positions", "appearances", "total.goals", "total.assists", "total.minutes.played", "contract.left.month","transferage",
"league", "Status", "searchresults","transferage_sq")
## Removing observations where contract lenght is unknown
transfer.data=filter(transfer.data , is.na(contract.left.month) == FALSE) 

##================ 4.1 Dividing into a train and test sample  ================
## Creating a vector with the count of 70 pct. of the sample size  
train_size = floor(0.70 * nrow(transfer.data)) # creates a vector 
## setting seed to enable reproductivity 
set.seed(123)
## creating a vector with random numbers (count = tran_size)
train.indicator = sample(seq_len(nrow(transfer.data)), size = train_size)
## Splitting the data frame into a train (70 pct.) and test sample (30 pct.)
train_sample = transfer.data[train.indicator,predicting.var] # selecting observations with a train indicator
test_sample = transfer.data[-train.indicator, predicting.var] # selecting observations without a train indicator
## creating a function that calculate the RMSE
get.rmse = function(real, estimate){
  return(sqrt(mean((real - estimate)^2)))
}
##================ 4.3 Baseline model: Simple average from training sample  ================
estimate_M1 = mean(train_sample$transfer.fee) #calculating estimate from model 1
get.rmse(test_sample$transfer.fee, estimate_M1) # calculating RMSE from estimate on test sample 
#create new data frame
train_sample.1<- train_sample %>% 
  select(transfer.fee,league) 
train_sample.1<- train_sample.1%>% 
  mutate(index=1:258)
#creating GGplot for visualisation
p = ggplot(train_sample.1, aes(x = index , y = transfer.fee))+
  geom_segment(aes(x= index, xend=index, y=transfer.fee, yend=estimate_M1), color="red") +
  geom_point(aes(x = index, y = transfer.fee, color = "black"))   +
  geom_line(aes(x = index, y = estimate_M1), color="green", size =1)+
  theme(axis.title.x=element_blank(),
        axis.text.x =element_blank(),
        axis.ticks= element_line(color=NA),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank())
```

```{r pressure, echo=FALSE}
p
```

## Ordinary least square
As the second prediction model we use the linear regression model - ordinary least square (OLS). OLS works by minimizing the *sum of squared residuals* (SSR) and is thereby also minimizing the bias. Unfortunately, the goal of minimizing the in-sample errors lead to a risk of overfitting and thereby increase the errors due to variance. Therefore, OLS is often not the best prediction methods. On the other hand OLS is very good method when inference is the goal because the model significance can be tested. When we apply our OLS model on the test sample we receive a RMSE of 6.34.

## The Lasso model
One way to deal with the overfitting/complexity problem described earlier is the Lasso model. The Lasso model punishes comlpexity by including a loss function in the OLS minimazation problem:  
$$ \operatorname*{minimize}_{\beta_j}: \sum_{i=1}^n (y_i-\beta_0-\sum_{i=1}^p\beta_jx_{ij}^2)+\lambda\sum_{j=1}^p|\beta_j| $$
From the equation above we see a clear trade-off between minimizing the SSR and the penalty term. The penalty is given by the sum of the absolute $\beta$ coefficients. For a given $\lambda$ the model returns a corner solution of the most significant variables. The $\lambda$ parameter weighs the penalty for model complexity, the larger $\lambda$, the heavier penalty and thereby exclusion of more variables. The Lasso is performing so-called *variable selection*. 
The optimal size of $\lambda$ is decided by running the regression on our training data for different values of $\lambda$. For each of the estimated models we estimate the transfer fee for our test data. We then calculate the RMSE for all the models and find the $\lambda$ which minimizes the RMSE in our test data.
The model with the optimal $\lambda$ has a RMSE of 6.34. Overfitting doesn't seem to be an issue for our OLS estimate since the weighting of the penalty term is very low ($\alpha=0.0009$) and the Lasso gives a higher out of sample error.


## Regression Trees
Decision trees is machine learning method which can be applied on both categorical and continuous variables. When used for predicting continuous outcomes it is called *regression trees*. The decision tree is grown by using our training data in the following way: First the predictor space (set of all the possible values for our explanatory variables) is divided into $J$ different regions. For all observations in one region the prediction is given by the mean of the observed dependent variable. The regions are constructed in a way that minimizes the SSR:
$$ \sum_{j=1}^J\sum_{i \in R_j}(y_i-\hat{y}_{R_m})^2$$

It is computationally infeasible to consider all possible partition of the feature spaces into $J$ boxes why an a approach called *recursive binary splitting* is used (*An Introduction to Statistical Learning*). In our case the first split is searchresults>50300 which means that this division of the observations provides the greatest reduction of the SSR of all possible first splits across all predictors and all values.  
There is a risk that this procedure will lead to overfitting. To avoid this we can use a procedure called pruning. To prune the tree we use the following equation:
$$ \sum_{m=1}^{|T|}\sum_{x_i \in R_j}(y_i-\hat{y}_{R_j})^2+\alpha|T|  $$
$|T|$ is the number of terminal nodes in the subtree $T$, $R_m$ is the subspace of the region M, and $y_{R_m}$ is the predicted response associated with $R_m$. When pruning the tree we obtain a sequence of the best subtrees as a function of $\alpha$. We are then using K-fold cross-validation (that is we are dividing the training data into K folds) to choose the right $\alpha$ value.
A new tree is grown on all but the *k*th fold of the training data, this is done for each value of $\alpha$. All the grown trees are evaluated by calculating the MSE on the *k*th fold. The $\alpha$ value with the lowest average MSE across the folds are choosen. The subtree from the original tree corresponding to the found $\alpha$ is the best when pruning.
As with the Lasso model overfitting wasn't an issue and the original tree produced a lower RMSE than the pruned. The RMSE was 6.60.
  
```{r fig.width=7, fig.height=7, fig.pos="placeHere",fig.cap="Decision Tree",echo=FALSE, message=FALSE, warning=FALSE}
library("tree")
library(caret)
library(plotly)
library(ggplot2)
library(glmnet)
library(dplyr)
library(plotly)
## Loading the final data set
transfer.data = read.csv("https://raw.githubusercontent.com/basgpol/SDS-group12/master/Exam_project/transferdata.final.csv", encoding = "UTF8", header = TRUE)
## Creating a new variable which is age squared
transfer.data$transferage_sq = transfer.data$transferage^2


## creating a vector with selected predictors for transferfee ()
predicting.var = c("transfer.fee", "positions", "appearances", "total.goals", "total.assists", 
               "total.minutes.played", "contract.left.month","transferage",
               "league", "Status", "searchresults","transferage_sq")
## Removing observations where contract lenght is unknown
transfer.data=filter(transfer.data , is.na(contract.left.month) == FALSE) 


##================ 4.1 Dividing into a train and test sample  ================

## Creating a vector with the count of 70 pct. of the sample size  
train_size = floor(0.70 * nrow(transfer.data)) # creates a vector 

## setting seed to enable reproductivity 
set.seed(123)

## creating a vector with random numbers (count = tran_size)
train.indicator = sample(seq_len(nrow(transfer.data)), size = train_size)

## Splitting the data frame into a train (70 pct.) and test sample (30 pct.)
train_sample = transfer.data[train.indicator,predicting.var] # selecting observations with a train indicator
test_sample = transfer.data[-train.indicator, predicting.var] # selecting observations without a train indicator



Model_4=tree(transfer.fee~.,data=train_sample, method="anova")

plot(Model_4,niform=TRUE, 
     main="Regression Tree for Transfer fee ")
text(Model_4,pretty=0,use.n=TRUE, cex=.5)

```


The figure illustrates the regression tree made to predict the transfer fee. The first node assign observations with less than 50300 search results to the left branch. The next node of the left branch assign players with less than 31.885 month of the contract left to the left branch and so on. In our model the tree segments the players into 11 leaves and predicts a predicts a transfer fee for each of these subgroups. The variables at the top of the tree is the most important one and the lenght of a branch shows the relative importance of the split. Hence, in our model search results is the most important predictor of a players transfer fee.

## Random Forest
As described above the decision tree has issues with overfitting which create errors due to variance. The issues can be reduced by another prediction method called *random forest* (RF). The RF is a socalled ensemple model which means that the model consist of several smaller decision trees (in this case regression trees). RF has great simalarities with the concept of *bagging* or *bottstrap aggregating* (Breiman, 1996) where the core idea is to reduce the error due to variance from one model by  building several models and use their average prediction. Random forest prediction can by divided into 4 steps:  
1) First, each regression tree is construced my randomly draw and replace (called bootstrap sampling) 63.2 percent of the observations in the training sample. This is in our rapport done 500 (?) times so we in the end have a forest of 500 different trees with random selected observations from the traning sample.    
2) In the next step each tree randomly select *m* number of the predictor variables in the data set. All 500 trees use the same number of variables. The purpose of randomly choose predictors is to prevent very important variables from overshadowing the effect of weaker variables. This often happens because the underlying algorithm search for the split with the largest decrease in the loss function.  
3) Thirdly, each tree calculate the out of bag error rate using the remaning 36.8 percent of the data.  
4) In the last step the average prediction are calculated out of all predictions from each individual tree.

In theory, RF does not need a seperate test sample to examine the validity of the results. The validity is measured internally by the out of bag errer rate. In this rapport we have used a seperate test sample. The reason is that we wanted to calculate the RMSE so we can compare the RF model's predictive power to the previous prediction models.  
The RMSE of the RF on the test sample is 6.26.

We can also use the RF to calculate and rank the importance of each predictor in the data set. This is done by calculating* %IncMSE*. The %IncMSE states the percentage increase of the mean squared error (estimated with the out-of-bag error rate) if the particular variable is permuted (which mean that the values are randomly shuffled). The higher the number the more important is the variable. The %IncMSE of diffent predictors are listed in the table below.  

```{r, include = FALSE}
Model_5 = randomForest(transfer.fee ~ ., data = train_sample, importance = TRUE)
print(Model_5)

estimate_M5 = predict(Model_5, test_sample) # calculating estimate from model 5
get.rmse(test_sample$transfer.fee, estimate_M5) # calculating the RMSE on test sample

var.list = importance(Model_5, type = 1) #calculate the variables influence
varImpPlot(Model_5) # plots the variables influence

```
```{r, include= TRUE}
var.list
```

# Conclusion
(Write conclusion)
